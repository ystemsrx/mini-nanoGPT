[English](https://github.com/ystemsrx/mini-nanoGPT) | [ç®€ä½“ä¸­æ–‡](README.zh.md)

# Mini NanoGPT ğŸš€

#### Is Training a GPT Really This Simple?

> Make GPT model training simple and fun! A visual training platform based on [karpathy/nanoGPT](https://github.com/karpathy/nanoGPT).

## ğŸ“– What Is This?

Mini-NanoGPT is a tool that helps you easily get started with GPT models. Whether you are:
- ğŸ“ A deep learning beginner
- ğŸ‘¨â€ğŸ”¬ A researcher
- ğŸ› ï¸ A developer

Or just someone interested in experiencing the charm of large models,

You can train models through a simple graphical interface!

> For the initial version of Mini NanoGPT (No longer updated), please check the [**old** branch](https://github.com/ystemsrx/mini-nanoGPT/tree/old)

## âœ¨ Key Features

### 1. Simple and Easy to Use
- ğŸ“± **Visual Interface**: Say goodbye to the command line and complete training with just a few clicks.
- ğŸŒ **Bilingual (Chinese and English)**: Full support for both Chinese and English interfaces.
- ğŸ¯ **One-Click Operations**: Data processing, training, and text generation can all be done with a single click.

### 2. Powerful Functionality
- ğŸ”¤ **Flexible Tokenization**: Supports character-level, GPT-2, or Qwen tokenizers, with multilingual support.
- ğŸš„ **Efficient Training**: Supports multi-processing acceleration and distributed training.
- ğŸ“Š **Real-Time Feedback**: Displays training progress and results in real time.
- âš™ï¸ **Parameter Visualization**: All training parameters can be directly adjusted in the interface.
- ğŸ§© **Database management**: Easier model management, saving training parameters at any time for next use.

## ğŸš€ Quick Start

### 1. Set Up the Environment
```bash
# Clone the repository
git clone --depth 1 https://github.com/ystemsrx/mini-nanoGPT.git
cd mini-nanogpt

# Install dependencies (Python 3.7+)
pip install -r requirements.txt
```

### 2. Launch the Project
```bash
python main.py
```
Open your browser and visit the displayed link to see the training interface! (Usually http://localhost:7860)

## ğŸ® User Guide

### Step 1: Prepare Data
- Open the "Data Processing" page, select or paste your training text, and choose the tokenization method. For better results, you can check the option to use a tokenizer, which will automatically build a vocabulary based on your text content.
- If you do not want to use a validation set for now, you can check "Do not use a validation set."
- After completion, click "Start Processing."
  
  Here's an example using a small piece of text:
  
![image](https://github.com/user-attachments/assets/667d1fb4-9f9a-4d3a-8574-894be7c71bc6)


### Step 2: Train the Model
- Switch to the "Training" page and adjust the parameters as needed (if you just want to experience it, you can keep the default values).
- The program supports real-time display of loss curves for the training set and validation set. If you generated a validation set in Step 1, there should theoretically be two curves below: the blue one for the training set loss and the orange one for the validation set loss.
- If only one curve is displayed, please check the terminal output. If you see output similar to:
  ```
  Error while evaluating val loss: Dataset too small: minimum dataset(val) size is 147, but block size is 512. Either reduce block size or add more data.
  ```
  It means that the block size you set is larger than your validation set. Please reduce its size, for example, to 128.
- This way, you should be able to see two dynamically changing curves normally.
- Click "Start Training" and wait for the model training to complete.
  
![image](https://github.com/user-attachments/assets/61b1f55e-5a9e-45e4-af9e-0c58f8a2be7e)


#### Evaluation-Only Mode?
- This mode allows you to evaluate the model's loss on the validation set. Set the `Number of Evaluation Seeds` to any value greater than 0 to enable evaluation-only mode. You can see the model's loss with different seeds.

### Step 3: Generate Text
1. Go to the "Inference" page
2. Enter an opening text
3. Click "Generate" to see what the model writes!

![image](https://github.com/user-attachments/assets/dcebc48a-69c2-4008-b6b4-3fec060a75fb)


## ğŸ“ Project Structure
```
mini-nanogpt/
â”œâ”€â”€ main.py          # Launch program
â”œâ”€â”€ src/             # Configuration files and other modules
â”œâ”€â”€ data/            # Data storage
â”œâ”€â”€ out/             # Model weights
â””â”€â”€ assets/          # Tokenizer files, etc.   
```

## â“ Frequently Asked Questions

### What if it's running too slowly?
- ğŸ’¡ Reduce the `batch_size` or model size.
- ğŸ’¡ Using a GPU will significantly speed up the process.
- ğŸ’¡ Increase the evaluation interval.

### The generated text isn't good enough?
- ğŸ’¡ Try increasing the amount of training data.
- ğŸ’¡ Adjust the model parameters appropriately.
- ğŸ’¡ Change the temperature parameter during generation.

### Want to continue previous training?
- ğŸ’¡ On the "Training" page, select "resume" in the "Initialization Method."
- ğŸ’¡ Specify the previous output directory.

## ğŸ¤ Contributing
Suggestions and improvements are welcome! You can contribute in the following ways:
- Submit an Issue
- Submit a Pull Request
- Share your usage experience

## ğŸ“ License
This project is open-sourced under the [MIT License](LICENSE).

---

ğŸ‰ **Start Your GPT Journey Now!**
