[English](README.md) | [ç®€ä½“ä¸­æ–‡](README.zh.md)

æ­¤ç›®å½•ä¸‹æ”¾çš„æ–‡ä»¶ç”¨äºåœ¨å®¹å™¨ä¸­è¿è¡Œã€åˆå§‹åŒ–ã€‚

# Mini-NanoGPT Docker éƒ¨ç½²æŒ‡å—

æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„ Docker è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒè‡ªåŠ¨æ£€æµ‹ CUDA å¹¶é€‰æ‹©ç›¸åº”çš„ PyTorch ç¯å¢ƒã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨ Docker Composeï¼ˆæ¨èï¼‰

```bash
# å‰å°å¯åŠ¨å®¹å™¨
docker-compose up --build

# åœæ­¢æœåŠ¡
docker-compose down
```

### ä½¿ç”¨ Docker å‘½ä»¤

```bash
# æ„å»ºé•œåƒ
docker build -t mini-nanogpt .

# è¿è¡Œå®¹å™¨ï¼ˆè‡ªåŠ¨æ£€æµ‹GPUï¼‰
docker run --gpus all -p 7860:7860 -v $(pwd)/data:/app/data mini-nanogpt

# è¿è¡Œå®¹å™¨ï¼ˆä»…CPUæ¨¡å¼ï¼‰
docker run -p 7860:7860 -v $(pwd)/data:/app/data mini-nanogpt
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **GPU ä¸è¢«è¯†åˆ«**
   ```bash
   # æ£€æŸ¥ NVIDIA é©±åŠ¨
   nvidia-smi
   
   # æ£€æŸ¥ Docker GPU æ”¯æŒ
   docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi
   ```

2. **ç«¯å£è¢«å ç”¨**
   ```bash
   # ä¿®æ”¹ docker-compose.yml ä¸­çš„ç«¯å£æ˜ å°„
   ports:
     - "8080:7860"  # ä½¿ç”¨ 8080 ç«¯å£
   ```

3. **å†…å­˜ä¸è¶³**
   ```bash
   # æ£€æŸ¥ç³»ç»Ÿèµ„æº
   docker stats
   
   # é™åˆ¶å®¹å™¨å†…å­˜ä½¿ç”¨
   docker run -m 4g mini-nanogpt
   ```

### æŸ¥çœ‹æ—¥å¿—
```bash
# Docker Compose æ—¥å¿—
docker-compose logs -f

# Docker å®¹å™¨æ—¥å¿—
docker logs mini-nanogpt
```

## ğŸ”„ æ›´æ–°å’Œç»´æŠ¤

```bash
# é‡æ–°æ„å»ºé•œåƒ
docker-compose build --no-cache

# æ¸…ç†æœªä½¿ç”¨çš„é•œåƒ
docker image prune

# å®Œå…¨é‡ç½®
docker-compose down
docker system prune -a
```

## ğŸ“ ç¯å¢ƒå˜é‡

å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰é…ç½®ï¼š

```yaml
environment:
  - GRADIO_SERVER_NAME=0.0.0.0
  - GRADIO_SERVER_PORT=7860
  - PYTHONUNBUFFERED=1
  - MINI_NANOGPT_ENV_TYPE=AUTO  # AUTO, CUDA, CPU
```
